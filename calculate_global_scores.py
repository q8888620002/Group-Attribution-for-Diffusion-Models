"""Calculate model behavior scores for diffusion models."""
import argparse
import json
import os
import torch

import constants

from pytorch_fid import fid_score

from utils import print_args, get_max_steps


def parse_args():
    """Parse command line arguments."""
    parser = argparse.ArgumentParser(description="Calculate model behavior scores")
    parser.add_argument(
        "--sample_dir",
        type=str,
        help="directory path of samples generated by a model",
        default=None
    )
    parser.add_argument(
        "--reference_dir",
        type=str,
        help="directory path of reference samples, from a dataset or a diffusion model",
        required=True,
    )
    parser.add_argument(
        "--outdir", type=str, help="results parent directory", default=constants.OUTDIR
    )
    parser.add_argument(
        "--dataset",
        type=str,
        help="dataset for training or unlearning",
        choices=["mnist", "cifar", "celeba", "imagenette"],
        default="mnist",
    )
    parser.add_argument(
        "--db",
        type=str,
        help="filepath of database for recording scores",
        required=True,
    )
    parser.add_argument(
        "--excluded_class",
        type=int,
        help="dataset class to exclude for class-wise data removal",
        default=None,
    )
    parser.add_argument(
        "--removal_dist",
        type=str,
        help="distribution for removing data",
        default=None,
    )
    parser.add_argument(
        "--datamodel_alpha",
        type=float,
        help="proportion of full dataset to keep in the datamodel distribution",
        default=0.5,
    )
    parser.add_argument(
        "--removal_seed",
        type=int,
        help="random seed for sampling from the removal distribution",
        default=0,
    )
    parser.add_argument(
        "--method",
        type=str,
        help="training or unlearning method",
        choices=["retrain", "gd", "ga", "esd"],
        required=True,
    )
    parser.add_argument(
        "--exp_name",
        type=str,
        help="experiment name to record in the database file",
        default=None,
    )
    parser.add_argument(
        "--batch_size",
        type=int,
        help="batch size for computation",
        default=512,
    )
    parser.add_argument(
        "--device", type=str, help="device used for computation", default="cuda:0"
    )
    parser.add_argument(
        "--use_ema",
        help="whether to use the EMA model",
        action="store_true",
        default=False,
    )
    args = parser.parse_args()
    return args


def main(args):
    """Main function for calculating global model behaviors."""

    sample_dir = args.sample_dir

    if not sample_dir:
        removal_dir = "full"
        if args.excluded_class is not None:
            removal_dir = f"excluded_{args.excluded_class}"
        if args.removal_dist is not None:
            removal_dir = f"{args.removal_dist}/{args.removal_dist}"
            if args.removal_dist == "datamodel":
                removal_dir += f"_alpha={args.datamodel_alpha}"
            removal_dir += f"_seed={args.removal_seed}"

        model_loaddir = os.path.join(
            args.outdir,
            args.dataset,
            args.method,
            "models",
            removal_dir,
        )
        sample_dir = os.path.join(
            args.outdir,
            args.dataset,
            args.method,
            "ema_generated_samples" if args.use_ema else "generated_samples",
            removal_dir,
        )

        existing_steps = get_max_steps(model_loaddir)
        if existing_steps is not None:
            ckpt_path = os.path.join(model_loaddir, f"ckpt_steps_{existing_steps:0>8}.pt")
            ckpt = torch.load(ckpt_path, map_location="cpu")
            remaining_idx = ckpt["remaining_idx"]
            removed_idx = ckpt["removed_idx"]

    info_dict = vars(args)
    # Check if subdirectories exist for conditional image generation.
    subdir_list = [
        entry
        for entry in os.listdir(sample_dir)
        if os.path.isdir(os.path.join(sample_dir, entry))
    ]
    if len(subdir_list) == 0:
        # Aggregate FID score. This is the standard practice even for conditional image
        # generation. For example, see
        # https://huggingface.co/docs/diffusers/main/en/conceptual/evaluation#class-conditioned-image-generation
        print("Calculating the FID score...")
        fid_value = fid_score.calculate_fid_given_paths(
            paths=[sample_dir, args.reference_dir],
            batch_size=args.batch_size,
            device=args.device,
            dims=2048,
        )
        fid_value_str = f"{fid_value:.4f}"

        # TODO: Calculate Precision and Recall to capture generated image fidelity and
        # diversity, respectively.

        print(f"FID score: {fid_value_str}")
        info_dict["fid_value"] = fid_value_str

    else:
        # Class-wise FID scores. If each class has too few reference samples, the
        # scores can be unstable.
        avg_fid_value = 0
        for subdir in subdir_list:
            print(f"Calculating the FID score for class {subdir}...")
            fid_value = fid_score.calculate_fid_given_paths(
                paths=[
                    os.path.join(sample_dir, subdir),
                    os.path.join(args.reference_dir, subdir),
                ],
                batch_size=args.batch_size,
                device=args.device,
                dims=2048,
            )
            fid_value_str = f"{fid_value:.4f}"
            avg_fid_value += fid_value

            print(f"FID score for {subdir}: {fid_value_str}")
            info_dict[f"fid_value/{subdir}"] = fid_value_str

        avg_fid_value /= len(subdir_list)
        avg_fid_value_str = f"{avg_fid_value:.4f}"
        print(f"Average FID score: {avg_fid_value_str}")
        info_dict["avg_fid_value"] = avg_fid_value_str

    info_dict["sample_dir"] = sample_dir
    info_dict["remaining_idx"] = ckpt["remaining_idx"].numpy().tolist()
    info_dict["removed_idx"] = ckpt["removed_idx"].numpy().tolist()

    with open(args.db, "a+") as f:
        f.write(json.dumps(info_dict) + "\n")
    print(f"Results saved to the database at {args.db}")


if __name__ == "__main__":
    args = parse_args()
    print_args(args)
    main(args)
    print("Done!")
