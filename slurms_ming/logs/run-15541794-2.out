--dataset cifar --device cuda:0 --load /gscratch/aims/diffusion-attr/results_ming/cifar/retrain/models/1 --method retrain --excluded_class 1
[rank: 0] Seed set to 42
wandb: Currently logged in as: mingyulu (data_att). Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.16.1
wandb: Run data is saved locally in /mmfs1/home/mingyulu/data_attribution/wandb/run-20231209_164736-wagqaoqk
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run misunderstood-capybara-120
wandb: ‚≠êÔ∏è View project at https://wandb.ai/data_att/Data%20Shapley%20for%20Diffusion
wandb: üöÄ View run at https://wandb.ai/data_att/Data%20Shapley%20for%20Diffusion/runs/wagqaoqk
Running main_new.py with arguments
	load=/gscratch/aims/diffusion-attr/results_ming/cifar/retrain/models/1
	init=False
	dataset=cifar
	log_freq=20
	excluded_class=1
	method=retrain
	opt_seed=42
	device=cuda:0
	outdir=/gscratch/aims/diffusion-attr/results_ming
	db=None
	exp_name=None
	resume=False
	lr_scheduler=constant
	lr_warmup_steps=0
	adam_beta1=0.9
	adam_beta2=0.999
	adam_weight_decay=0.0
	adam_epsilon=1e-08
	ema_inv_gamma=1.0
	ema_power=0.75
	ema_max_decay=0.9999
	num_inference_steps=100
	num_train_steps=1000
Files already downloaded and verified
Files already downloaded and verified
Loading pruned/pretrained model from /gscratch/aims/diffusion-attr/results_ming/cifar/retrain/models/1
Epoch[1/800], Step[20/352], steps_time: 8.028, loss: 0.02672, gradient norms: 0.14209, parameters norms: 253.68488, lr: 0.000100
Epoch[1/800], Step[40/352], steps_time: 5.136, loss: 0.03099, gradient norms: 0.05307, parameters norms: 253.69771, lr: 0.000100
Epoch[1/800], Step[60/352], steps_time: 5.140, loss: 0.02173, gradient norms: 0.04327, parameters norms: 253.70949, lr: 0.000100
Epoch[1/800], Step[80/352], steps_time: 5.151, loss: 0.02547, gradient norms: 0.04955, parameters norms: 253.71947, lr: 0.000100
Epoch[1/800], Step[100/352], steps_time: 5.159, loss: 0.02648, gradient norms: 0.01883, parameters norms: 253.72948, lr: 0.000100
Epoch[1/800], Step[120/352], steps_time: 5.171, loss: 0.03750, gradient norms: 0.02306, parameters norms: 253.74051, lr: 0.000100
Epoch[1/800], Step[140/352], steps_time: 5.337, loss: 0.03694, gradient norms: 0.03831, parameters norms: 253.75075, lr: 0.000100
Epoch[1/800], Step[160/352], steps_time: 5.175, loss: 0.03270, gradient norms: 0.03817, parameters norms: 253.76154, lr: 0.000100
Epoch[1/800], Step[180/352], steps_time: 5.187, loss: 0.03528, gradient norms: 0.04502, parameters norms: 253.77032, lr: 0.000100
Epoch[1/800], Step[200/352], steps_time: 5.190, loss: 0.03352, gradient norms: 0.03139, parameters norms: 253.77826, lr: 0.000100
Epoch[1/800], Step[220/352], steps_time: 5.192, loss: 0.02517, gradient norms: 0.05007, parameters norms: 253.78914, lr: 0.000100
Epoch[1/800], Step[240/352], steps_time: 5.195, loss: 0.03387, gradient norms: 0.03755, parameters norms: 253.79890, lr: 0.000100
Epoch[1/800], Step[260/352], steps_time: 5.198, loss: 0.02575, gradient norms: 0.03466, parameters norms: 253.80916, lr: 0.000100
Epoch[1/800], Step[280/352], steps_time: 5.197, loss: 0.02496, gradient norms: 0.05482, parameters norms: 253.81888, lr: 0.000100
Epoch[1/800], Step[300/352], steps_time: 5.197, loss: 0.03177, gradient norms: 0.08982, parameters norms: 253.82872, lr: 0.000100
Epoch[1/800], Step[320/352], steps_time: 5.199, loss: 0.02640, gradient norms: 0.04118, parameters norms: 253.83856, lr: 0.000100
Epoch[1/800], Step[340/352], steps_time: 5.199, loss: 0.03319, gradient norms: 0.06058, parameters norms: 253.84784, lr: 0.000100
/mmfs1/gscratch/cse/mingyulu/gstratch/cse/mingyulu/miniconda3/envs/data_att/lib/python3.8/site-packages/torch/nn/modules/conv.py:456: UserWarning: Applied workaround for CuDNN issue, install nvrtc.so (Triggered internally at /opt/conda/conda-bld/pytorch_1695392020195/work/aten/src/ATen/native/cudnn/Conv_v8.cpp:80.)
  return F.conv2d(input, weight, bias, self.stride,
/mmfs1/gscratch/cse/mingyulu/gstratch/cse/mingyulu/miniconda3/envs/data_att/lib/python3.8/site-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
Epoch[2/800], Step[20/352], steps_time: 5.383, loss: 0.02649, gradient norms: 0.12498, parameters norms: 253.86398, lr: 0.000100
Epoch[2/800], Step[40/352], steps_time: 5.200, loss: 0.02208, gradient norms: 0.04628, parameters norms: 253.87277, lr: 0.000100
Epoch[2/800], Step[60/352], steps_time: 5.202, loss: 0.03793, gradient norms: 0.03401, parameters norms: 253.88179, lr: 0.000100
Epoch[2/800], Step[80/352], steps_time: 5.200, loss: 0.02370, gradient norms: 0.05392, parameters norms: 253.89093, lr: 0.000100
Epoch[2/800], Step[100/352], steps_time: 5.202, loss: 0.03312, gradient norms: 0.05292, parameters norms: 253.89984, lr: 0.000100
Epoch[2/800], Step[120/352], steps_time: 5.203, loss: 0.02393, gradient norms: 0.10783, parameters norms: 253.90904, lr: 0.000100
Epoch[2/800], Step[140/352], steps_time: 5.205, loss: 0.02141, gradient norms: 0.03823, parameters norms: 253.91766, lr: 0.000100
Epoch[2/800], Step[160/352], steps_time: 5.210, loss: 0.02490, gradient norms: 0.03698, parameters norms: 253.92725, lr: 0.000100
Epoch[2/800], Step[180/352], steps_time: 5.214, loss: 0.02223, gradient norms: 0.06900, parameters norms: 253.93596, lr: 0.000100
Epoch[2/800], Step[200/352], steps_time: 5.213, loss: 0.02451, gradient norms: 0.02501, parameters norms: 253.94507, lr: 0.000100
Epoch[2/800], Step[220/352], steps_time: 5.213, loss: 0.02738, gradient norms: 0.06759, parameters norms: 253.95425, lr: 0.000100
Epoch[2/800], Step[240/352], steps_time: 5.216, loss: 0.03475, gradient norms: 0.02691, parameters norms: 253.96222, lr: 0.000100
Epoch[2/800], Step[260/352], steps_time: 5.213, loss: 0.02697, gradient norms: 0.07297, parameters norms: 253.97099, lr: 0.000100
Epoch[2/800], Step[280/352], steps_time: 5.212, loss: 0.02806, gradient norms: 0.05004, parameters norms: 253.98019, lr: 0.000100
Epoch[2/800], Step[300/352], steps_time: 5.216, loss: 0.02132, gradient norms: 0.05902, parameters norms: 253.98915, lr: 0.000100
Epoch[2/800], Step[320/352], steps_time: 5.215, loss: 0.02569, gradient norms: 0.07169, parameters norms: 253.99763, lr: 0.000100
Epoch[2/800], Step[340/352], steps_time: 5.214, loss: 0.01827, gradient norms: 0.03230, parameters norms: 254.00729, lr: 0.000100
Epoch[3/800], Step[20/352], steps_time: 5.425, loss: 0.03335, gradient norms: 0.06797, parameters norms: 254.01918, lr: 0.000100
Epoch[3/800], Step[40/352], steps_time: 5.237, loss: 0.03066, gradient norms: 0.05987, parameters norms: 254.02844, lr: 0.000100
Epoch[3/800], Step[60/352], steps_time: 5.378, loss: 0.03158, gradient norms: 0.04580, parameters norms: 254.03712, lr: 0.000100
Epoch[3/800], Step[80/352], steps_time: 5.236, loss: 0.02494, gradient norms: 0.09006, parameters norms: 254.04543, lr: 0.000100
Epoch[3/800], Step[100/352], steps_time: 5.219, loss: 0.02825, gradient norms: 0.06815, parameters norms: 254.05486, lr: 0.000100
Epoch[3/800], Step[120/352], steps_time: 5.220, loss: 0.03026, gradient norms: 0.03641, parameters norms: 254.06369, lr: 0.000100
Epoch[3/800], Step[140/352], steps_time: 5.218, loss: 0.02142, gradient norms: 0.06972, parameters norms: 254.07295, lr: 0.000100
Epoch[3/800], Step[160/352], steps_time: 5.217, loss: 0.03082, gradient norms: 0.08723, parameters norms: 254.08150, lr: 0.000100
Epoch[3/800], Step[180/352], steps_time: 5.217, loss: 0.03163, gradient norms: 0.05858, parameters norms: 254.09064, lr: 0.000100
Epoch[3/800], Step[200/352], steps_time: 5.218, loss: 0.02950, gradient norms: 0.04930, parameters norms: 254.09933, lr: 0.000100
Epoch[3/800], Step[220/352], steps_time: 5.218, loss: 0.02537, gradient norms: 0.03347, parameters norms: 254.10670, lr: 0.000100
