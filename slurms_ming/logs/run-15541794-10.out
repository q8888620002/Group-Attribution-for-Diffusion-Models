--dataset cifar --device cuda:0 --load /gscratch/aims/diffusion-attr/results_ming/cifar/retrain/models/9 --method retrain --excluded_class 9
[rank: 0] Seed set to 42
wandb: Currently logged in as: mingyulu (data_att). Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.16.1
wandb: Run data is saved locally in /mmfs1/home/mingyulu/data_attribution/wandb/run-20231209_164840-yxm4ypb3
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ethereal-snowflake-124
wandb: ‚≠êÔ∏è View project at https://wandb.ai/data_att/Data%20Shapley%20for%20Diffusion
wandb: üöÄ View run at https://wandb.ai/data_att/Data%20Shapley%20for%20Diffusion/runs/yxm4ypb3
Running main_new.py with arguments
	load=/gscratch/aims/diffusion-attr/results_ming/cifar/retrain/models/9
	init=False
	dataset=cifar
	log_freq=20
	excluded_class=9
	method=retrain
	opt_seed=42
	device=cuda:0
	outdir=/gscratch/aims/diffusion-attr/results_ming
	db=None
	exp_name=None
	resume=False
	lr_scheduler=constant
	lr_warmup_steps=0
	adam_beta1=0.9
	adam_beta2=0.999
	adam_weight_decay=0.0
	adam_epsilon=1e-08
	ema_inv_gamma=1.0
	ema_power=0.75
	ema_max_decay=0.9999
	num_inference_steps=100
	num_train_steps=1000
Files already downloaded and verified
Files already downloaded and verified
Loading pruned/pretrained model from /gscratch/aims/diffusion-attr/results_ming/cifar/retrain/models/9
Epoch[1/800], Step[20/352], steps_time: 7.276, loss: 0.02830, gradient norms: 0.08143, parameters norms: 243.28136, lr: 0.000100
Epoch[1/800], Step[40/352], steps_time: 5.121, loss: 0.02806, gradient norms: 0.03400, parameters norms: 243.29497, lr: 0.000100
Epoch[1/800], Step[60/352], steps_time: 5.132, loss: 0.02230, gradient norms: 0.05737, parameters norms: 243.30707, lr: 0.000100
Epoch[1/800], Step[80/352], steps_time: 5.142, loss: 0.02667, gradient norms: 0.05861, parameters norms: 243.31674, lr: 0.000100
Epoch[1/800], Step[100/352], steps_time: 5.155, loss: 0.02251, gradient norms: 0.02275, parameters norms: 243.32672, lr: 0.000100
Epoch[1/800], Step[120/352], steps_time: 5.167, loss: 0.03839, gradient norms: 0.04471, parameters norms: 243.33765, lr: 0.000100
Epoch[1/800], Step[140/352], steps_time: 5.337, loss: 0.03508, gradient norms: 0.03814, parameters norms: 243.34814, lr: 0.000100
Epoch[1/800], Step[160/352], steps_time: 5.183, loss: 0.03145, gradient norms: 0.08084, parameters norms: 243.35844, lr: 0.000100
Epoch[1/800], Step[180/352], steps_time: 5.194, loss: 0.03667, gradient norms: 0.06507, parameters norms: 243.36858, lr: 0.000100
Epoch[1/800], Step[200/352], steps_time: 5.198, loss: 0.03630, gradient norms: 0.04290, parameters norms: 243.37823, lr: 0.000100
Epoch[1/800], Step[220/352], steps_time: 5.206, loss: 0.02563, gradient norms: 0.04573, parameters norms: 243.38794, lr: 0.000100
Epoch[1/800], Step[240/352], steps_time: 5.210, loss: 0.03157, gradient norms: 0.03788, parameters norms: 243.39919, lr: 0.000100
Epoch[1/800], Step[260/352], steps_time: 5.208, loss: 0.02626, gradient norms: 0.04226, parameters norms: 243.40817, lr: 0.000100
Epoch[1/800], Step[280/352], steps_time: 5.213, loss: 0.02600, gradient norms: 0.04131, parameters norms: 243.41733, lr: 0.000100
Epoch[1/800], Step[300/352], steps_time: 5.215, loss: 0.03112, gradient norms: 0.07262, parameters norms: 243.42685, lr: 0.000100
Epoch[1/800], Step[320/352], steps_time: 5.219, loss: 0.02733, gradient norms: 0.03027, parameters norms: 243.43510, lr: 0.000100
Epoch[1/800], Step[340/352], steps_time: 5.223, loss: 0.03179, gradient norms: 0.02724, parameters norms: 243.44467, lr: 0.000100
/mmfs1/gscratch/cse/mingyulu/gstratch/cse/mingyulu/miniconda3/envs/data_att/lib/python3.8/site-packages/torch/nn/modules/conv.py:456: UserWarning: Applied workaround for CuDNN issue, install nvrtc.so (Triggered internally at /opt/conda/conda-bld/pytorch_1695392020195/work/aten/src/ATen/native/cudnn/Conv_v8.cpp:80.)
  return F.conv2d(input, weight, bias, self.stride,
/mmfs1/gscratch/cse/mingyulu/gstratch/cse/mingyulu/miniconda3/envs/data_att/lib/python3.8/site-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
Epoch[2/800], Step[20/352], steps_time: 5.412, loss: 0.02540, gradient norms: 0.07338, parameters norms: 243.46036, lr: 0.000100
Epoch[2/800], Step[40/352], steps_time: 5.226, loss: 0.01975, gradient norms: 0.05451, parameters norms: 243.47043, lr: 0.000100
Epoch[2/800], Step[60/352], steps_time: 5.227, loss: 0.03066, gradient norms: 0.01579, parameters norms: 243.47923, lr: 0.000100
Epoch[2/800], Step[80/352], steps_time: 5.228, loss: 0.02320, gradient norms: 0.06895, parameters norms: 243.48811, lr: 0.000100
Epoch[2/800], Step[100/352], steps_time: 5.224, loss: 0.03244, gradient norms: 0.03787, parameters norms: 243.49820, lr: 0.000100
Epoch[2/800], Step[120/352], steps_time: 5.231, loss: 0.02791, gradient norms: 0.09291, parameters norms: 243.50681, lr: 0.000100
Epoch[2/800], Step[140/352], steps_time: 5.224, loss: 0.02062, gradient norms: 0.07632, parameters norms: 243.51610, lr: 0.000100
Epoch[2/800], Step[160/352], steps_time: 5.230, loss: 0.02528, gradient norms: 0.08987, parameters norms: 243.52539, lr: 0.000100
Epoch[2/800], Step[180/352], steps_time: 5.229, loss: 0.02251, gradient norms: 0.02287, parameters norms: 243.53497, lr: 0.000100
Epoch[2/800], Step[200/352], steps_time: 5.226, loss: 0.02431, gradient norms: 0.03290, parameters norms: 243.54370, lr: 0.000100
Epoch[2/800], Step[220/352], steps_time: 5.229, loss: 0.02955, gradient norms: 0.07170, parameters norms: 243.55321, lr: 0.000100
Epoch[2/800], Step[240/352], steps_time: 5.227, loss: 0.03517, gradient norms: 0.04109, parameters norms: 243.56320, lr: 0.000100
Epoch[2/800], Step[260/352], steps_time: 5.229, loss: 0.02576, gradient norms: 0.14724, parameters norms: 243.57150, lr: 0.000100
Epoch[2/800], Step[280/352], steps_time: 5.228, loss: 0.02681, gradient norms: 0.04539, parameters norms: 243.57971, lr: 0.000100
Epoch[2/800], Step[300/352], steps_time: 5.227, loss: 0.01969, gradient norms: 0.06811, parameters norms: 243.58847, lr: 0.000100
Epoch[2/800], Step[320/352], steps_time: 5.228, loss: 0.02580, gradient norms: 0.02811, parameters norms: 243.59784, lr: 0.000100
