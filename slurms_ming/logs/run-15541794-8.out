--dataset cifar --device cuda:0 --load /gscratch/aims/diffusion-attr/results_ming/cifar/retrain/models/7 --method retrain --excluded_class 7
[rank: 0] Seed set to 42
wandb: Currently logged in as: mingyulu (data_att). Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.16.1
wandb: Run data is saved locally in /mmfs1/home/mingyulu/data_attribution/wandb/run-20231209_164733-33457yn7
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ancient-durian-117
wandb: ‚≠êÔ∏è View project at https://wandb.ai/data_att/Data%20Shapley%20for%20Diffusion
wandb: üöÄ View run at https://wandb.ai/data_att/Data%20Shapley%20for%20Diffusion/runs/33457yn7
Running main_new.py with arguments
	load=/gscratch/aims/diffusion-attr/results_ming/cifar/retrain/models/7
	init=False
	dataset=cifar
	log_freq=20
	excluded_class=7
	method=retrain
	opt_seed=42
	device=cuda:0
	outdir=/gscratch/aims/diffusion-attr/results_ming
	db=None
	exp_name=None
	resume=False
	lr_scheduler=constant
	lr_warmup_steps=0
	adam_beta1=0.9
	adam_beta2=0.999
	adam_weight_decay=0.0
	adam_epsilon=1e-08
	ema_inv_gamma=1.0
	ema_power=0.75
	ema_max_decay=0.9999
	num_inference_steps=100
	num_train_steps=1000
Files already downloaded and verified
Files already downloaded and verified
Loading pruned/pretrained model from /gscratch/aims/diffusion-attr/results_ming/cifar/retrain/models/7
Epoch[1/800], Step[20/352], steps_time: 9.951, loss: 0.02752, gradient norms: 0.03787, parameters norms: 253.00279, lr: 0.000100
Epoch[1/800], Step[40/352], steps_time: 5.253, loss: 0.03233, gradient norms: 0.07158, parameters norms: 253.01605, lr: 0.000100
Epoch[1/800], Step[60/352], steps_time: 5.292, loss: 0.02092, gradient norms: 0.04168, parameters norms: 253.02780, lr: 0.000100
Epoch[1/800], Step[80/352], steps_time: 5.328, loss: 0.02610, gradient norms: 0.04859, parameters norms: 253.03775, lr: 0.000100
Epoch[1/800], Step[100/352], steps_time: 5.388, loss: 0.02553, gradient norms: 0.02416, parameters norms: 253.04785, lr: 0.000100
Epoch[1/800], Step[120/352], steps_time: 5.428, loss: 0.04185, gradient norms: 0.02238, parameters norms: 253.05836, lr: 0.000100
Epoch[1/800], Step[140/352], steps_time: 5.598, loss: 0.03454, gradient norms: 0.02783, parameters norms: 253.06902, lr: 0.000100
Epoch[1/800], Step[160/352], steps_time: 5.475, loss: 0.03603, gradient norms: 0.05979, parameters norms: 253.07881, lr: 0.000100
Epoch[1/800], Step[180/352], steps_time: 5.494, loss: 0.03624, gradient norms: 0.06626, parameters norms: 253.08897, lr: 0.000100
Epoch[1/800], Step[200/352], steps_time: 5.502, loss: 0.03460, gradient norms: 0.07368, parameters norms: 253.09940, lr: 0.000100
Epoch[1/800], Step[220/352], steps_time: 5.514, loss: 0.02594, gradient norms: 0.04200, parameters norms: 253.10959, lr: 0.000100
Epoch[1/800], Step[240/352], steps_time: 5.524, loss: 0.03944, gradient norms: 0.02694, parameters norms: 253.11916, lr: 0.000100
Epoch[1/800], Step[260/352], steps_time: 5.530, loss: 0.02563, gradient norms: 0.07523, parameters norms: 253.12962, lr: 0.000100
Epoch[1/800], Step[280/352], steps_time: 5.533, loss: 0.02798, gradient norms: 0.04980, parameters norms: 253.13959, lr: 0.000100
Epoch[1/800], Step[300/352], steps_time: 5.538, loss: 0.03213, gradient norms: 0.09063, parameters norms: 253.14935, lr: 0.000100
Epoch[1/800], Step[320/352], steps_time: 5.541, loss: 0.02344, gradient norms: 0.05447, parameters norms: 253.15814, lr: 0.000100
Epoch[1/800], Step[340/352], steps_time: 5.547, loss: 0.03040, gradient norms: 0.05185, parameters norms: 253.16721, lr: 0.000100
/mmfs1/gscratch/cse/mingyulu/gstratch/cse/mingyulu/miniconda3/envs/data_att/lib/python3.8/site-packages/torch/nn/modules/conv.py:456: UserWarning: Applied workaround for CuDNN issue, install nvrtc.so (Triggered internally at /opt/conda/conda-bld/pytorch_1695392020195/work/aten/src/ATen/native/cudnn/Conv_v8.cpp:80.)
  return F.conv2d(input, weight, bias, self.stride,
/mmfs1/gscratch/cse/mingyulu/gstratch/cse/mingyulu/miniconda3/envs/data_att/lib/python3.8/site-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
Epoch[2/800], Step[20/352], steps_time: 5.731, loss: 0.02660, gradient norms: 0.06676, parameters norms: 253.18291, lr: 0.000100
Epoch[2/800], Step[40/352], steps_time: 5.551, loss: 0.01897, gradient norms: 0.01566, parameters norms: 253.19231, lr: 0.000100
Epoch[2/800], Step[60/352], steps_time: 5.553, loss: 0.03733, gradient norms: 0.03113, parameters norms: 253.20172, lr: 0.000100
Epoch[2/800], Step[80/352], steps_time: 5.555, loss: 0.02199, gradient norms: 0.05459, parameters norms: 253.21088, lr: 0.000100
Epoch[2/800], Step[100/352], steps_time: 5.554, loss: 0.03369, gradient norms: 0.04506, parameters norms: 253.21927, lr: 0.000100
Epoch[2/800], Step[120/352], steps_time: 5.552, loss: 0.02690, gradient norms: 0.11085, parameters norms: 253.22823, lr: 0.000100
Epoch[2/800], Step[140/352], steps_time: 5.552, loss: 0.02383, gradient norms: 0.04348, parameters norms: 253.23685, lr: 0.000100
Epoch[2/800], Step[160/352], steps_time: 5.549, loss: 0.02368, gradient norms: 0.06435, parameters norms: 253.24564, lr: 0.000100
Epoch[2/800], Step[180/352], steps_time: 5.554, loss: 0.02477, gradient norms: 0.02960, parameters norms: 253.25418, lr: 0.000100
Epoch[2/800], Step[200/352], steps_time: 5.550, loss: 0.02222, gradient norms: 0.07261, parameters norms: 253.26320, lr: 0.000100
Epoch[2/800], Step[220/352], steps_time: 5.551, loss: 0.02840, gradient norms: 0.06449, parameters norms: 253.27216, lr: 0.000100
Epoch[2/800], Step[240/352], steps_time: 5.550, loss: 0.03263, gradient norms: 0.07327, parameters norms: 253.28148, lr: 0.000100
Epoch[2/800], Step[260/352], steps_time: 5.548, loss: 0.02514, gradient norms: 0.08391, parameters norms: 253.28970, lr: 0.000100
Epoch[2/800], Step[280/352], steps_time: 5.543, loss: 0.02535, gradient norms: 0.07383, parameters norms: 253.29729, lr: 0.000100
Epoch[2/800], Step[300/352], steps_time: 5.546, loss: 0.02269, gradient norms: 0.07833, parameters norms: 253.30638, lr: 0.000100
Epoch[2/800], Step[320/352], steps_time: 5.547, loss: 0.02549, gradient norms: 0.07125, parameters norms: 253.31500, lr: 0.000100
Epoch[2/800], Step[340/352], steps_time: 5.545, loss: 0.01866, gradient norms: 0.02020, parameters norms: 253.32431, lr: 0.000100
Epoch[3/800], Step[20/352], steps_time: 5.694, loss: 0.03235, gradient norms: 0.02875, parameters norms: 253.33839, lr: 0.000100
Epoch[3/800], Step[40/352], steps_time: 5.543, loss: 0.02892, gradient norms: 0.04943, parameters norms: 253.34615, lr: 0.000100
Epoch[3/800], Step[60/352], steps_time: 5.674, loss: 0.02919, gradient norms: 0.03558, parameters norms: 253.35493, lr: 0.000100
Epoch[3/800], Step[80/352], steps_time: 5.538, loss: 0.02340, gradient norms: 0.02383, parameters norms: 253.36360, lr: 0.000100
Epoch[3/800], Step[100/352], steps_time: 5.539, loss: 0.03014, gradient norms: 0.04818, parameters norms: 253.37244, lr: 0.000100
Epoch[3/800], Step[120/352], steps_time: 5.539, loss: 0.02803, gradient norms: 0.06099, parameters norms: 253.38084, lr: 0.000100
Epoch[3/800], Step[140/352], steps_time: 5.539, loss: 0.02210, gradient norms: 0.08635, parameters norms: 253.38951, lr: 0.000100
Epoch[3/800], Step[160/352], steps_time: 5.537, loss: 0.03107, gradient norms: 0.04083, parameters norms: 253.39859, lr: 0.000100
