--dataset cifar --device cuda:0 --load /gscratch/aims/diffusion-attr/results_ming/cifar/retrain/models/3 --method retrain --excluded_class 3
[rank: 0] Seed set to 42
wandb: Currently logged in as: mingyulu (data_att). Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.16.1
wandb: Run data is saved locally in /mmfs1/home/mingyulu/data_attribution/wandb/run-20231209_164736-hmlabmdm
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run drawn-haze-120
wandb: ‚≠êÔ∏è View project at https://wandb.ai/data_att/Data%20Shapley%20for%20Diffusion
wandb: üöÄ View run at https://wandb.ai/data_att/Data%20Shapley%20for%20Diffusion/runs/hmlabmdm
Running main_new.py with arguments
	load=/gscratch/aims/diffusion-attr/results_ming/cifar/retrain/models/3
	init=False
	dataset=cifar
	log_freq=20
	excluded_class=3
	method=retrain
	opt_seed=42
	device=cuda:0
	outdir=/gscratch/aims/diffusion-attr/results_ming
	db=None
	exp_name=None
	resume=False
	lr_scheduler=constant
	lr_warmup_steps=0
	adam_beta1=0.9
	adam_beta2=0.999
	adam_weight_decay=0.0
	adam_epsilon=1e-08
	ema_inv_gamma=1.0
	ema_power=0.75
	ema_max_decay=0.9999
	num_inference_steps=100
	num_train_steps=1000
Files already downloaded and verified
Files already downloaded and verified
Loading pruned/pretrained model from /gscratch/aims/diffusion-attr/results_ming/cifar/retrain/models/3
Epoch[1/800], Step[20/352], steps_time: 8.066, loss: 0.03032, gradient norms: 0.06173, parameters norms: 254.94290, lr: 0.000100
Epoch[1/800], Step[40/352], steps_time: 5.177, loss: 0.03357, gradient norms: 0.03812, parameters norms: 254.95650, lr: 0.000100
Epoch[1/800], Step[60/352], steps_time: 5.195, loss: 0.02277, gradient norms: 0.07195, parameters norms: 254.96828, lr: 0.000100
Epoch[1/800], Step[80/352], steps_time: 5.206, loss: 0.02948, gradient norms: 0.08607, parameters norms: 254.97816, lr: 0.000100
Epoch[1/800], Step[100/352], steps_time: 5.221, loss: 0.02751, gradient norms: 0.04068, parameters norms: 254.98819, lr: 0.000100
Epoch[1/800], Step[120/352], steps_time: 5.230, loss: 0.03809, gradient norms: 0.03562, parameters norms: 254.99919, lr: 0.000100
Epoch[1/800], Step[140/352], steps_time: 5.407, loss: 0.03022, gradient norms: 0.05934, parameters norms: 255.00940, lr: 0.000100
Epoch[1/800], Step[160/352], steps_time: 5.252, loss: 0.03659, gradient norms: 0.05691, parameters norms: 255.01955, lr: 0.000100
Epoch[1/800], Step[180/352], steps_time: 5.256, loss: 0.03331, gradient norms: 0.05695, parameters norms: 255.02954, lr: 0.000100
Epoch[1/800], Step[200/352], steps_time: 5.260, loss: 0.04084, gradient norms: 0.04804, parameters norms: 255.03955, lr: 0.000100
Epoch[1/800], Step[220/352], steps_time: 5.263, loss: 0.02587, gradient norms: 0.06564, parameters norms: 255.04939, lr: 0.000100
Epoch[1/800], Step[240/352], steps_time: 5.274, loss: 0.03625, gradient norms: 0.03032, parameters norms: 255.05910, lr: 0.000100
Epoch[1/800], Step[260/352], steps_time: 5.277, loss: 0.02760, gradient norms: 0.02416, parameters norms: 255.06868, lr: 0.000100
Epoch[1/800], Step[280/352], steps_time: 5.275, loss: 0.02663, gradient norms: 0.03408, parameters norms: 255.07759, lr: 0.000100
Epoch[1/800], Step[300/352], steps_time: 5.280, loss: 0.03084, gradient norms: 0.05213, parameters norms: 255.08737, lr: 0.000100
Epoch[1/800], Step[320/352], steps_time: 5.283, loss: 0.02504, gradient norms: 0.03778, parameters norms: 255.09679, lr: 0.000100
Epoch[1/800], Step[340/352], steps_time: 5.281, loss: 0.03419, gradient norms: 0.03030, parameters norms: 255.10593, lr: 0.000100
/mmfs1/gscratch/cse/mingyulu/gstratch/cse/mingyulu/miniconda3/envs/data_att/lib/python3.8/site-packages/torch/nn/modules/conv.py:456: UserWarning: Applied workaround for CuDNN issue, install nvrtc.so (Triggered internally at /opt/conda/conda-bld/pytorch_1695392020195/work/aten/src/ATen/native/cudnn/Conv_v8.cpp:80.)
  return F.conv2d(input, weight, bias, self.stride,
/mmfs1/gscratch/cse/mingyulu/gstratch/cse/mingyulu/miniconda3/envs/data_att/lib/python3.8/site-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
Epoch[2/800], Step[20/352], steps_time: 5.492, loss: 0.02642, gradient norms: 0.04414, parameters norms: 255.12064, lr: 0.000100
Epoch[2/800], Step[40/352], steps_time: 5.302, loss: 0.02246, gradient norms: 0.05540, parameters norms: 255.12978, lr: 0.000100
Epoch[2/800], Step[60/352], steps_time: 5.304, loss: 0.03697, gradient norms: 0.03599, parameters norms: 255.13841, lr: 0.000100
Epoch[2/800], Step[80/352], steps_time: 5.306, loss: 0.02262, gradient norms: 0.04019, parameters norms: 255.14897, lr: 0.000100
Epoch[2/800], Step[100/352], steps_time: 5.300, loss: 0.03237, gradient norms: 0.03650, parameters norms: 255.15758, lr: 0.000100
Epoch[2/800], Step[120/352], steps_time: 5.290, loss: 0.02626, gradient norms: 0.07764, parameters norms: 255.16608, lr: 0.000100
Epoch[2/800], Step[140/352], steps_time: 5.290, loss: 0.02391, gradient norms: 0.03810, parameters norms: 255.17532, lr: 0.000100
Epoch[2/800], Step[160/352], steps_time: 5.291, loss: 0.02308, gradient norms: 0.06299, parameters norms: 255.18317, lr: 0.000100
Epoch[2/800], Step[180/352], steps_time: 5.291, loss: 0.02337, gradient norms: 0.05856, parameters norms: 255.19156, lr: 0.000100
Epoch[2/800], Step[200/352], steps_time: 5.293, loss: 0.02322, gradient norms: 0.05059, parameters norms: 255.19978, lr: 0.000100
Epoch[2/800], Step[220/352], steps_time: 5.289, loss: 0.03150, gradient norms: 0.04094, parameters norms: 255.20868, lr: 0.000100
Epoch[2/800], Step[240/352], steps_time: 5.293, loss: 0.03524, gradient norms: 0.03986, parameters norms: 255.21785, lr: 0.000100
Epoch[2/800], Step[260/352], steps_time: 5.290, loss: 0.02645, gradient norms: 0.09506, parameters norms: 255.22688, lr: 0.000100
Epoch[2/800], Step[280/352], steps_time: 5.287, loss: 0.02889, gradient norms: 0.08289, parameters norms: 255.23480, lr: 0.000100
Epoch[2/800], Step[300/352], steps_time: 5.290, loss: 0.02033, gradient norms: 0.09935, parameters norms: 255.24286, lr: 0.000100
Epoch[2/800], Step[320/352], steps_time: 5.294, loss: 0.02815, gradient norms: 0.03994, parameters norms: 255.25169, lr: 0.000100
Epoch[2/800], Step[340/352], steps_time: 5.292, loss: 0.01889, gradient norms: 0.05371, parameters norms: 255.26070, lr: 0.000100
Epoch[3/800], Step[20/352], steps_time: 5.462, loss: 0.03325, gradient norms: 0.02106, parameters norms: 255.27451, lr: 0.000100
Epoch[3/800], Step[40/352], steps_time: 5.290, loss: 0.03158, gradient norms: 0.05210, parameters norms: 255.28342, lr: 0.000100
Epoch[3/800], Step[60/352], steps_time: 5.424, loss: 0.02812, gradient norms: 0.03548, parameters norms: 255.29178, lr: 0.000100
Epoch[3/800], Step[80/352], steps_time: 5.290, loss: 0.02437, gradient norms: 0.05231, parameters norms: 255.30038, lr: 0.000100
Epoch[3/800], Step[100/352], steps_time: 5.287, loss: 0.02910, gradient norms: 0.07306, parameters norms: 255.30972, lr: 0.000100
Epoch[3/800], Step[120/352], steps_time: 5.288, loss: 0.02978, gradient norms: 0.04621, parameters norms: 255.31837, lr: 0.000100
Epoch[3/800], Step[140/352], steps_time: 5.289, loss: 0.01997, gradient norms: 0.05284, parameters norms: 255.32712, lr: 0.000100
Epoch[3/800], Step[160/352], steps_time: 5.291, loss: 0.03184, gradient norms: 0.04650, parameters norms: 255.33531, lr: 0.000100
Epoch[3/800], Step[180/352], steps_time: 5.287, loss: 0.03102, gradient norms: 0.06963, parameters norms: 255.34297, lr: 0.000100
Epoch[3/800], Step[200/352], steps_time: 5.293, loss: 0.03079, gradient norms: 0.05588, parameters norms: 255.35179, lr: 0.000100
